{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from networks.RotCAtt_TransUNet_plusplus_gradcam import RotCAtt_TransUNet_plusplus_GradCam\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from networks.dense_feature_extraction import Dense\n",
    "from networks.linear_embedding import LinearEmbedding\n",
    "from networks.transformer import Transformer\n",
    "from networks.rotatory_attention import RotatoryAttention\n",
    "from networks.recon import Reconstruction\n",
    "from networks.uct_decoder import UCTDecoder\n",
    "from networks.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_path = 'outputs/RotCAtt_TransUNet_plusplus/VHSCDD_RotCAtt_TransUNet_plusplus_bs6_ps16_epo600_hw512_ly4/model.pth'\n",
    "        self.trained_model = torch.load(self.model_path)\n",
    "        self.config = get_config() \n",
    "        self.dense = Dense(self.config).cuda()\n",
    "        self.linear_embedding = LinearEmbedding(self.config).cuda()\n",
    "        self.transformer = Transformer(self.config).cuda()\n",
    "        self.rotatory_attention = RotatoryAttention(self.config).cuda()\n",
    "        self.reconstruct = Reconstruction(self.config).cuda()\n",
    "        self.decoder = UCTDecoder(self.config).cuda()\n",
    "        self.out = nn.Conv2d(self.config.df[0], self.config.num_classes, kernel_size=(1,1), stride=(1,1)).cuda()\n",
    "\n",
    "        # define state dict\n",
    "        dense_state_dict = self.dense.state_dict()\n",
    "        embedding_state_dict = self.linear_embedding.state_dict()\n",
    "        transformer_state_dict = self.transformer.state_dict()\n",
    "        rot_state_dict = self.rotatory_attention.state_dict()\n",
    "        recon_state_dict = self.reconstruct.state_dict()\n",
    "        decoder_state_dict = self.decoder.state_dict()\n",
    "        out_state_dict = self.out.state_dict()  \n",
    "\n",
    "        for name, param in self.trained_model.state_dict().items():\n",
    "            if name.startswith('dense'):\n",
    "                dense_state_dict[name[len(\"dense.\"):]].copy_(param)\n",
    "            elif name.startswith('linear_embedding'):\n",
    "                embedding_state_dict[name[len(\"linear_embedding.\"):]].copy_(param)\n",
    "            elif name.startswith('transformer'):\n",
    "                transformer_state_dict[name[len('transformer.'):]].copy_(param)\n",
    "            elif name.startswith('rotatory_attention'):\n",
    "                rot_state_dict[name[len('rotatory_attention.'):]].copy_(param)\n",
    "            elif name.startswith('reconstruct'):\n",
    "                recon_state_dict[name[len('reconstruct.'):]].copy_(param)\n",
    "            elif name.startswith('decoder'):\n",
    "                decoder_state_dict[name[len('decoder.'):]].copy_(param)\n",
    "            elif name.startswith('out'):\n",
    "                out_state_dict[name[len('out.'):]].copy_(param)\n",
    "\n",
    "        self.dense.eval()\n",
    "        self.linear_embedding.eval()\n",
    "        self.transformer.eval()\n",
    "        self.rotatory_attention.eval()\n",
    "        self.rotatory_attention.eval()\n",
    "        self.reconstruct.eval()\n",
    "        self.decoder.eval()\n",
    "        self.out.eval()\n",
    "        self.gradients = []\n",
    "        \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def clear_activations_gradient(self):\n",
    "        self.gradients.clear() \n",
    "        \n",
    "    def get_activations(self, x):\n",
    "        x1, x2, x3, x4 = self.dense(x)\n",
    "        z1, z2, z3 = self.linear_embedding(x1, x2, x3)\n",
    "        e1, e2, e3, a1_weights, a2_weights, a3_weights = self.transformer(z1, z2, z3)\n",
    "        r1, r2, r3 = self.rotatory_attention(z1, z2, z3)\n",
    "\n",
    "        f1 = e1 + r1\n",
    "        f2 = e2 + r2\n",
    "        f3 = e3 + r3\n",
    "\n",
    "        o1, o2, o3 = self.reconstruct(f1, f2, f3)\n",
    "        y = self.decoder(o1, o2, o3, x4)\n",
    "        return y\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4 = self.dense(x)\n",
    "        z1, z2, z3 = self.linear_embedding(x1, x2, x3)\n",
    "        e1, e2, e3, a1_weights, a2_weights, a3_weights = self.transformer(z1, z2, z3)\n",
    "        r1, r2, r3 = self.rotatory_attention(z1, z2, z3)\n",
    "\n",
    "        f1 = e1 + r1\n",
    "        f2 = e2 + r2\n",
    "        f3 = e3 + r3\n",
    "        \n",
    "        o1, o2, o3 = self.reconstruct(f1, f2, f3)\n",
    "        y = self.decoder(o1, o2, o3, x4)\n",
    "        y.register_hook(self.activations_hook)\n",
    "        return self.out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'data/VHSCDD_512/test_images/0001.nii.gz'\n",
    "model = RotModel()\n",
    "\n",
    "num_slice      = 4\n",
    "class_instance = 6\n",
    "index_list     = [125, 156, 153,  180]\n",
    "instance_list  = [3, 5, 11, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(index_list) == len(instance_list), print(\n",
    "        \"Length of list of indices is not equal to length of list of instances\")\n",
    "    \n",
    "name_dict = {\n",
    "    0: \"background\",\n",
    "    1: \"left_ventricle\",\n",
    "    2: \"right_ventricle\",\n",
    "    3: \"left_atrium\",\n",
    "    4: \"right_atrium\",\n",
    "    5:  \"myocardium\",\n",
    "    6: \"descending_aeorta\",\n",
    "    7: \"pulmonary_trunk\",\n",
    "    8: \"ascending_aorta\",\n",
    "    9: \"vena_cava\",\n",
    "    10: \"auricle\",\n",
    "    11: \"coronary_artery\",\n",
    "}\n",
    "\n",
    "num_slice = len(index_list)\n",
    "image = sitk.GetArrayFromImage(sitk.ReadImage(image_path, sitk.sitkFloat32))\n",
    "\n",
    "output = np.ones((num_slice, 512, 512), dtype=np.float32)\n",
    "slice_list = np.array([image[index] for index in index_list])\n",
    "\n",
    "for i in range(num_slice): output[i,:,:] = slice_list[i]\n",
    "\n",
    "input = torch.from_numpy(output).unsqueeze(1).cuda()\n",
    "output = model(input)\n",
    "activations = model.get_activations(input).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = cv2.COLORMAP_JET\n",
    "fig, axes = plt.subplots(nrows=2, ncols=num_slice, figsize=(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate score\n",
    "for x in range(num_slice):\n",
    "    class_output = output[x, instance_list[x]]\n",
    "    class_score_sum = class_output.sum()\n",
    "    class_score_sum.backward(retain_graph=True)\n",
    "    \n",
    "    class_output = output[x, class_instance]\n",
    "    class_score_sum = class_output.sum()\n",
    "    class_score_sum.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "    gradients = model.get_activations_gradient()\n",
    "    print(f\"Length gradients: {len(gradients)}\")\n",
    "    gradients = gradients[-1]\n",
    "    model.clear_activations_gradient()\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "    print(f\"Pooled Gradient: {pooled_gradients.shape}\")\n",
    "\n",
    "    instance_activation = activations[x]\n",
    "\n",
    "    for channel in range(64):\n",
    "        instance_activation[channel, :, :] *= pooled_gradients[channel]\n",
    "\n",
    "    print(f\"Activations: {instance_activation.shape}\")\n",
    "\n",
    "    heatmap = torch.mean(instance_activation, dim=0)\n",
    "    print(f\"Heatmap shape: {heatmap.shape}\")\n",
    "\n",
    "    heatmap /= torch.max(heatmap)\n",
    "    heatmap = heatmap.cpu().numpy()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "    heatmap = cv2.resize(heatmap, (512,512))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "\n",
    "    image0 = torch.squeeze(input[x, :, :, :])\n",
    "    image0 = image0.cpu().numpy()\n",
    "    image0 = cv2.cvtColor(image0, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    superimposed_img = (heatmap / 255.0) * 0.6 + image0\n",
    "\n",
    "    heatmap_plot = axes[0][x].imshow(superimposed_img, cmap='RdBu')\n",
    "    axes[0][x].set_xlabel(f\"Slice: {index_list[x]} || Class: {name_dict[instance_list[x]]}\")\n",
    "    fig.colorbar(heatmap_plot, ax=axes[0][x], fraction=0.046)\n",
    "\n",
    "    superimposed_plot = axes[1][x].imshow(heatmap, cmap='RdBu')\n",
    "    axes[1][x].set_xlabel(f\"Heatmap of slice: {index_list[x]}\")\n",
    "\n",
    "    fig.colorbar(superimposed_plot, ax=axes[1][x], fraction=0.046)\n",
    "\n",
    "title_text = \"\"\n",
    "for i, x in enumerate(index_list):\n",
    "    title_text += str(x)\n",
    "\n",
    "    if i == len(index_list) - 1: pass\n",
    "    else: title_text += \", \"\n",
    "\n",
    "plt.suptitle(f\"Model's focus on slices number: {title_text}\", fontsize=16)\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
